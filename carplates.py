# -*- coding: utf-8 -*-
"""CarPlates.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FfTdfnoGV-1JIHFgy7hDiokvthcRSPWK

# Train Model to detect Car Plates

### Load the Drive helper and mount (Optional)
"""

from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

"""### Process DataSet"""

import os
import torch
from torchvision import transforms
from PIL import Image, ImageFile


ImageFile.LOAD_TRUNCATED_IMAGES = True

# Define a simple transformation pipeline
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # For MobileNet
])

# Custom dataset class to load images and masks
class CarPlateDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, mask_dir, transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]
        self.transform = transform

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        image_path = os.path.join(self.image_dir, self.image_filenames[idx])
        mask_path = os.path.join(self.mask_dir, self.image_filenames[idx].replace(".jpg", "_mask.png"))

        # Load images
        image = Image.open(image_path).convert("RGB")
        mask = Image.open(mask_path).convert("L")  # Assuming masks are grayscale

        if self.transform:
            image = self.transform(image)
            mask = transforms.Resize((256, 256))(mask)
            mask = transforms.ToTensor()(mask)

        return image, mask

"""### Use MobileNet - U-Net"""

import torch.nn as nn
from torchvision import models

class MobileNetUNet(nn.Module):
    def __init__(self, num_classes=1):
        super(MobileNetUNet, self).__init__()
        self.backbone = models.mobilenet_v2(pretrained=True).features

        # Decoder layers
        self.decoder1 = self._decoder_block(1280, 512)
        self.decoder2 = self._decoder_block(512, 256)
        self.decoder3 = self._decoder_block(256, 128)
        self.decoder4 = self._decoder_block(128, 64)
        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)

    def _decoder_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        x1 = self.backbone[:4](x)  # MobileNet downsampling layers
        x2 = self.backbone[4:](x1)  # MobileNet deep layers

        x = self.decoder1(x2)
        x = self.decoder2(x)
        x = self.decoder3(x)
        x = self.decoder4(x)
        x = self.final_conv(x)
        return x

# Instantiate the model
model = MobileNetUNet(num_classes=1)  # for binary segmentation

!pip install tqdm

"""### Train Model"""

import torch
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import DataLoader, random_split
import torch.optim as optim

import torch.nn as nn
import torch.nn.functional as fun
from tqdm import tqdm


# Dataset and DataLoader
dataset = CarPlateDataset(image_dir="/content/drive/MyDrive/DataSet/Vehicles", mask_dir="/content/drive/MyDrive/DataSet/VehiclesMasks", transform=transform)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# Loss function and optimizer
criterion = BCEWithLogitsLoss()  # For binary segmentation
optimizer = optim.Adam(model.parameters(), lr=0.001)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Training loop
epochs = 25
for epoch in range(epochs):
    model.train()
    running_loss = 0.0

    # Wrap the DataLoader with tqdm
    with tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs}", unit="batch") as progress_bar:
        for images, masks in progress_bar:
            images, masks = images.to(device), masks.to(device)
            optimizer.zero_grad()

            # Forward pass
            model = model.to(device)
            outputs = model(images)

            # Resize masks to match output size
            masks_resized = fun.interpolate(masks, size=outputs.shape[2:], mode='bilinear', align_corners=False)

            # Compute loss
            loss = criterion(outputs, masks_resized)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # Update tqdm progress bar with current loss
            progress_bar.set_postfix(loss=loss.item())

    # Print average loss for the epoch
    print(f"Epoch {epoch + 1}, Average Loss: {running_loss / len(train_loader):.4f}")
    # print average dice coefficient for the epoch
    print(f"Epoch {epoch + 1}, Average Dice Coefficient: {1 - running_loss / len(train_loader):.4f}")

torch.save(model.state_dict(), f"mobilenet_unet.pth")

model.eval()
val_loss = 0.0
with torch.no_grad():
    for images, masks in val_loader:
        images, masks = images.to(device), masks.to(device)
        outputs = model(images)
        masks_resized = fun.interpolate(masks, size=outputs.shape[2:], mode='bilinear', align_corners=False)
        val_loss += criterion(outputs, masks_resized).item()

avg_val_loss = val_loss / len(val_loader)
print(f"Validation Loss: {avg_val_loss:.4f}")

image_path = "/content/drive/MyDrive/005.jpg"

"""### test model"""

import torch
from torchvision import transforms
from PIL import Image

import matplotlib.pyplot as plt


# Load the model
model = MobileNetUNet(num_classes=1)
model.load_state_dict(torch.load("/content/drive/MyDrive/mobilenet_unet.pth", map_location=torch.device('cpu')))
model.eval()

# Define preprocessing transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),  # Adjust to your model's input size
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization
])

# Load a sample image
image = Image.open(image_path).convert("RGB")
input_image = transform(image).unsqueeze(0)  # Add batch dimension

# Inference
with torch.no_grad():
    output = model(input_image)

# Process the output (e.g., apply a threshold for binary segmentation)
predicted_mask = torch.sigmoid(output).squeeze().cpu().numpy()  # Convert to numpy
predicted_mask = (predicted_mask > 0.5).astype("uint8")  # Binarize


# Display the input image and the predicted mask
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Input Image")
plt.imshow(image)
plt.axis("off")

plt.subplot(1, 2, 2)
plt.title("Predicted Mask")
plt.imshow(predicted_mask, cmap="gray")
plt.axis("off")

plt.show()

"""### Extract Region of interset"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the original image
image = cv2.imread(image_path)

# Scale mask if values are in [0, 1]
if predicted_mask.max() <= 1.0:
    predicted_mask = (predicted_mask * 255).astype(np.uint8)

# Resize mask to match image dimensions
predicted_mask = cv2.resize(predicted_mask, (image.shape[1], image.shape[0]))

# Ensure the mask is binary
predicted_mask = (predicted_mask > 127).astype(np.uint8) * 255

# Apply the mask to the original image
masked_image = cv2.bitwise_and(image, image, mask=predicted_mask)

# crop masked image
x, y, w, h = cv2.boundingRect(predicted_mask)
cropped_masked_image = masked_image[y:y+h, x:x+w]


# Visualize the results
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis("off")

plt.subplot(1, 2, 2)
plt.title("Masked Image")
plt.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))
plt.axis("off")

plt.tight_layout()
plt.show()

# save the masked image
cv2.imwrite("data/output/masked_image.jpg", cropped_masked_image)

"""Extract text From Image"""